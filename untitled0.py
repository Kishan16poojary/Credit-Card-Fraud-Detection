# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kzHYbSflyyOKdYKDvQj4F5hCeBXJbAdR
"""

import pandas as pd

# Load the file
df = pd.read_csv('creditcard.csv')

# Print the first 5 rows to make sure it's working
print("Success! Here is a look at your data:")
print(df.head())

# Check how many are fraud vs genuine
print(df['Class'].value_counts())

# Calculate the percentage
n_genuine = df['Class'].value_counts()[0]
n_fraud = df['Class'].value_counts()[1]

print(f"\nGenuine Transactions: {n_genuine}")
print(f"Fraudulent Transactions: {n_fraud}")
print(f"Percentage of Fraud: {(n_fraud/(n_genuine+n_fraud))*100:.2f}%")

from sklearn.preprocessing import StandardScaler

# 1. Initialize the scaler
scaler = StandardScaler()

# 2. Scale the 'Amount' column and create a new column 'scaled_amount'
# We drop the original 'Amount' and 'Time' because they aren't useful in their raw form
df['scaled_amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))

# 3. Drop the old columns
df.drop(['Time', 'Amount'], axis=1, inplace=True)

# 4. Show the new data structure
print("Data after scaling 'Amount':")
print(df.head())



from imblearn.over_sampling import SMOTE

# Check for any NaNs created during scaling/splitting
print("Missing values before fix:", df.isnull().sum().sum())

# Drop any rows that have missing values
df.dropna(inplace=True)

print("Missing values after fix:", df.isnull().sum().sum())

# 1. Separate Features and Target again with clean data
X = df.drop('Class', axis=1)
y = df['Class']

# 2. Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Apply SMOTE
from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# 4. Verify
print("Success! Balanced Fraud cases:", sum(y_train_res == 1))

from sklearn.ensemble import RandomForestClassifier

# 1. Initialize the Model
# We use a small number of 'trees' (n_estimators) to make it run faster for now
model = RandomForestClassifier(n_estimators=100, random_state=42)

# 2. Train the model using the BALANCED data
print("Training the model... Please wait a moment.")
model.fit(X_train_res, y_train_res)
print("Training Complete!")

# Use the model to predict on the test set
y_pred = model.predict(X_test)

print("Predictions are ready for evaluation.")

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Generate the Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# 2. Visualize it with a Heatmap
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Genuine', 'Fraud'], yticklabels=['Genuine', 'Fraud'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix: Fraud Detection Performance')
plt.show()

# 3. Print the detailed report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))